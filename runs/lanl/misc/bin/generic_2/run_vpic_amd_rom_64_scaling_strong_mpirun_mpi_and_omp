#!/bin/bash

##############################################################################################################################################
# Configure modules.

source bashrc.modules

##############################################################################################################################################
# Figure out problem being run based on value of PWD variable.

if echo $PWD | grep -q "lpi"
then
    prob=lpi
fi

if echo $PWD | grep -q "lpi_sp"
then
    prob=lpi_sp
fi

if echo $PWD | grep -q "reconnection"
then
    prob=reconnection
fi

if [ "x$prob" = "x" ]
then
    echo "The prob bash variable is not set to a valid problem."
    exit 1
fi

##############################################################################################################################################
# Preliminary stuff.

export OMPI_MCA_btl="^openib"

vtime=''

#voc='--vm-overcommit=enable'
voc=''

##############################################################################################################################################
# Configure aprun and xpre variables based on type of run.

############################################################
# Configure default run without extra tools.

xpre='./'

aprun='mpirun'

############################################################
# Configure run with mpitrace.

#xpre='./'

#aprun='mpirun'

#ARCH=DARWIN_AMD_EPC_OMPI_GNU_OPT_ARTEAM
#PROJECTS_ROOT=/home/wdn/Projects
#MPITRACE_PROJECT_ROOT=$PROJECTS_ROOT/mpitrace_walkup_project
#MPITRACE_ROOT=$MPITRACE_PROJECT_ROOT/build/$ARCH/mpitrace_walkup

#source $MPITRACE_ROOT/bashrc.modules

# Case 1
#export VPROF_PROFILE=yes
#export LD_PRELOAD=$MPITRACE_ROOT/src/libmpitrace.so

# Case 2
#export HPM_PROFILE_THRESHOLD=2000000
#export HPM_PROFILE_EVENT=""
#export LD_PRELOAD=$MPITRACE_ROOT/src/libmpihpm.so

# Case 3
#export HPM_PROFILE_THRESHOLD=2000000
#export HPM_PROFILE_EVENT=""
#export LD_PRELOAD=$MPITRACE_ROOT/hpmprof/libhpmprof.so

############################################################
# Configure MAP profiling collection run.

#xpre='./'

#aprun='map --profile mpirun'

############################################################
# Configure Intel Application Performance Snapshot run.

#xpre='aps ./'

#aprun='mpirun'

##############################################################################################################################################
# Define some useful variables to tame the command lines when using the low level process binding interface.

cpu_bind_rpc1_128="--map-by ppr:64:socket:pe=1"
cpu_bind_rpc1_064="--map-by ppr:32:socket:pe=1"
cpu_bind_rpc1_032="--map-by ppr:16:socket:pe=1"
cpu_bind_rpc1_016="--map-by ppr:8:socket:pe=1"
cpu_bind_rpc1_008="--map-by ppr:4:socket:pe=1"
cpu_bind_rpc1_004="--map-by ppr:2:socket:pe=1"
cpu_bind_rpc1_002="--map-by ppr:1:socket:pe=1"
cpu_bind_rpc1_001="--map-by ppr:1:socket:pe=1"

##############################################################################################################################################
# Large DDR problem, strong scaled across 1 node.

##############################################################################################################################################
# MPI + OpenMP, 1 rank/core, 2 threads/rank.

export OMP_NUM_THREADS=2

${aprun} -n 128 ${cpu_bind_rpc1_128} ${voc} ${vtime} ${xpre}${prob}_ddr_nn_0001_nppn_128_ntpp_002.Linux --tpp 2 >& ${prob}_ddr_nn_0001_nppn_128_ntpp_002.log
${aprun} -n  64 ${cpu_bind_rpc1_064} ${voc} ${vtime} ${xpre}${prob}_ddr_nn_0001_nppn_064_ntpp_002.Linux --tpp 2 >& ${prob}_ddr_nn_0001_nppn_064_ntpp_002.log
${aprun} -n  32 ${cpu_bind_rpc1_032} ${voc} ${vtime} ${xpre}${prob}_ddr_nn_0001_nppn_032_ntpp_002.Linux --tpp 2 >& ${prob}_ddr_nn_0001_nppn_032_ntpp_002.log
${aprun} -n  16 ${cpu_bind_rpc1_016} ${voc} ${vtime} ${xpre}${prob}_ddr_nn_0001_nppn_016_ntpp_002.Linux --tpp 2 >& ${prob}_ddr_nn_0001_nppn_016_ntpp_002.log
${aprun} -n   8 ${cpu_bind_rpc1_008} ${voc} ${vtime} ${xpre}${prob}_ddr_nn_0001_nppn_008_ntpp_002.Linux --tpp 2 >& ${prob}_ddr_nn_0001_nppn_008_ntpp_002.log
${aprun} -n   4 ${cpu_bind_rpc1_004} ${voc} ${vtime} ${xpre}${prob}_ddr_nn_0001_nppn_004_ntpp_002.Linux --tpp 2 >& ${prob}_ddr_nn_0001_nppn_004_ntpp_002.log
${aprun} -n   2 ${cpu_bind_rpc1_002} ${voc} ${vtime} ${xpre}${prob}_ddr_nn_0001_nppn_002_ntpp_002.Linux --tpp 2 >& ${prob}_ddr_nn_0001_nppn_002_ntpp_002.log
${aprun} -n   1 ${cpu_bind_rpc1_001} ${voc} ${vtime} ${xpre}${prob}_ddr_nn_0001_nppn_001_ntpp_002.Linux --tpp 2 >& ${prob}_ddr_nn_0001_nppn_001_ntpp_002.log

##############################################################################################################################################
# MPI + OpenMP, 1 rank/core, 1 thread/rank.

export OMP_NUM_THREADS=1

${aprun} -n 128 ${cpu_bind_rpc1_128} ${voc} ${vtime} ${xpre}${prob}_ddr_nn_0001_nppn_128_ntpp_001.Linux --tpp 1 >& ${prob}_ddr_nn_0001_nppn_128_ntpp_001.log
${aprun} -n  64 ${cpu_bind_rpc1_064} ${voc} ${vtime} ${xpre}${prob}_ddr_nn_0001_nppn_064_ntpp_001.Linux --tpp 1 >& ${prob}_ddr_nn_0001_nppn_064_ntpp_001.log
${aprun} -n  32 ${cpu_bind_rpc1_032} ${voc} ${vtime} ${xpre}${prob}_ddr_nn_0001_nppn_032_ntpp_001.Linux --tpp 1 >& ${prob}_ddr_nn_0001_nppn_032_ntpp_001.log
${aprun} -n  16 ${cpu_bind_rpc1_016} ${voc} ${vtime} ${xpre}${prob}_ddr_nn_0001_nppn_016_ntpp_001.Linux --tpp 1 >& ${prob}_ddr_nn_0001_nppn_016_ntpp_001.log
${aprun} -n   8 ${cpu_bind_rpc1_008} ${voc} ${vtime} ${xpre}${prob}_ddr_nn_0001_nppn_008_ntpp_001.Linux --tpp 1 >& ${prob}_ddr_nn_0001_nppn_008_ntpp_001.log
${aprun} -n   4 ${cpu_bind_rpc1_004} ${voc} ${vtime} ${xpre}${prob}_ddr_nn_0001_nppn_004_ntpp_001.Linux --tpp 1 >& ${prob}_ddr_nn_0001_nppn_004_ntpp_001.log
${aprun} -n   2 ${cpu_bind_rpc1_002} ${voc} ${vtime} ${xpre}${prob}_ddr_nn_0001_nppn_002_ntpp_001.Linux --tpp 1 >& ${prob}_ddr_nn_0001_nppn_002_ntpp_001.log
${aprun} -n   1 ${cpu_bind_rpc1_001} ${voc} ${vtime} ${xpre}${prob}_ddr_nn_0001_nppn_001_ntpp_001.Linux --tpp 1 >& ${prob}_ddr_nn_0001_nppn_001_ntpp_001.log

##############################################################################################################################################
# Small HBM problem, strong scaled across 1 node.

##############################################################################################################################################
# MPI + OpenMP, 1 rank/core, 2 threads/rank.

export OMP_NUM_THREADS=2

${aprun} -n 128 ${cpu_bind_rpc1_128} ${voc} ${vtime} ${xpre}${prob}_hbm_nn_0001_nppn_128_ntpp_002.Linux --tpp 2 >& ${prob}_hbm_nn_0001_nppn_128_ntpp_002.log
${aprun} -n  64 ${cpu_bind_rpc1_064} ${voc} ${vtime} ${xpre}${prob}_hbm_nn_0001_nppn_064_ntpp_002.Linux --tpp 2 >& ${prob}_hbm_nn_0001_nppn_064_ntpp_002.log
${aprun} -n  32 ${cpu_bind_rpc1_032} ${voc} ${vtime} ${xpre}${prob}_hbm_nn_0001_nppn_032_ntpp_002.Linux --tpp 2 >& ${prob}_hbm_nn_0001_nppn_032_ntpp_002.log
${aprun} -n  16 ${cpu_bind_rpc1_016} ${voc} ${vtime} ${xpre}${prob}_hbm_nn_0001_nppn_016_ntpp_002.Linux --tpp 2 >& ${prob}_hbm_nn_0001_nppn_016_ntpp_002.log
${aprun} -n   8 ${cpu_bind_rpc1_008} ${voc} ${vtime} ${xpre}${prob}_hbm_nn_0001_nppn_008_ntpp_002.Linux --tpp 2 >& ${prob}_hbm_nn_0001_nppn_008_ntpp_002.log
${aprun} -n   4 ${cpu_bind_rpc1_004} ${voc} ${vtime} ${xpre}${prob}_hbm_nn_0001_nppn_004_ntpp_002.Linux --tpp 2 >& ${prob}_hbm_nn_0001_nppn_004_ntpp_002.log
${aprun} -n   2 ${cpu_bind_rpc1_002} ${voc} ${vtime} ${xpre}${prob}_hbm_nn_0001_nppn_002_ntpp_002.Linux --tpp 2 >& ${prob}_hbm_nn_0001_nppn_002_ntpp_002.log
${aprun} -n   1 ${cpu_bind_rpc1_001} ${voc} ${vtime} ${xpre}${prob}_hbm_nn_0001_nppn_001_ntpp_002.Linux --tpp 2 >& ${prob}_hbm_nn_0001_nppn_001_ntpp_002.log

##############################################################################################################################################
# MPI + OpenMP, 1 rank/core, 1 thread/rank.

export OMP_NUM_THREADS=1

${aprun} -n 128 ${cpu_bind_rpc1_128} ${voc} ${vtime} ${xpre}${prob}_hbm_nn_0001_nppn_128_ntpp_001.Linux --tpp 1 >& ${prob}_hbm_nn_0001_nppn_128_ntpp_001.log
${aprun} -n  64 ${cpu_bind_rpc1_064} ${voc} ${vtime} ${xpre}${prob}_hbm_nn_0001_nppn_064_ntpp_001.Linux --tpp 1 >& ${prob}_hbm_nn_0001_nppn_064_ntpp_001.log
${aprun} -n  32 ${cpu_bind_rpc1_032} ${voc} ${vtime} ${xpre}${prob}_hbm_nn_0001_nppn_032_ntpp_001.Linux --tpp 1 >& ${prob}_hbm_nn_0001_nppn_032_ntpp_001.log
${aprun} -n  16 ${cpu_bind_rpc1_016} ${voc} ${vtime} ${xpre}${prob}_hbm_nn_0001_nppn_016_ntpp_001.Linux --tpp 1 >& ${prob}_hbm_nn_0001_nppn_016_ntpp_001.log
${aprun} -n   8 ${cpu_bind_rpc1_008} ${voc} ${vtime} ${xpre}${prob}_hbm_nn_0001_nppn_008_ntpp_001.Linux --tpp 1 >& ${prob}_hbm_nn_0001_nppn_008_ntpp_001.log
${aprun} -n   4 ${cpu_bind_rpc1_004} ${voc} ${vtime} ${xpre}${prob}_hbm_nn_0001_nppn_004_ntpp_001.Linux --tpp 1 >& ${prob}_hbm_nn_0001_nppn_004_ntpp_001.log
${aprun} -n   2 ${cpu_bind_rpc1_002} ${voc} ${vtime} ${xpre}${prob}_hbm_nn_0001_nppn_002_ntpp_001.Linux --tpp 1 >& ${prob}_hbm_nn_0001_nppn_002_ntpp_001.log
${aprun} -n   1 ${cpu_bind_rpc1_001} ${voc} ${vtime} ${xpre}${prob}_hbm_nn_0001_nppn_001_ntpp_001.Linux --tpp 1 >& ${prob}_hbm_nn_0001_nppn_001_ntpp_001.log
