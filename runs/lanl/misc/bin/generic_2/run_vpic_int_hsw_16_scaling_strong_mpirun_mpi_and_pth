#!/bin/bash

##############################################################################################################################################
# Configure modules.

source bashrc.modules

##############################################################################################################################################
# Figure out problem being run based on value of PWD variable.

if echo $PWD | grep -q "lpi"
then
    prob=lpi
fi

if echo $PWD | grep -q "lpi_sp"
then
    prob=lpi_sp
fi

if echo $PWD | grep -q "reconnection"
then
    prob=reconnection
fi

if [ "x$prob" = "x" ]
then
    echo "The prob bash variable is not set to a valid problem."
    exit 1
fi

##############################################################################################################################################
# Preliminary stuff.

export OMPI_MCA_btl="^openib"

vtime=''

#voc='--vm-overcommit=enable'
voc=''

##############################################################################################################################################
# Configure aprun and xpre variables based on type of run.

############################################################
# Configure default run without extra tools.

xpre='./'

aprun='mpirun'

############################################################
# Configure MAP profiling collection run.

#xpre='./'

#aprun='map --profile mpirun'

############################################################
# Configure Intel Application Performance Snapshot run.

#xpre='aps ./'

#aprun='mpirun'

##############################################################################################################################################
# Define some useful variables to tame the command lines when using the low level process binding interface.

cpu_bind_rpc1_32="--map-by ppr:16:socket:pe=1"
cpu_bind_rpc1_16="--map-by ppr:8:socket:pe=1"
cpu_bind_rpc1_08="--map-by ppr:4:socket:pe=1"
cpu_bind_rpc1_04="--map-by ppr:2:socket:pe=1"
cpu_bind_rpc1_02="--map-by ppr:1:socket:pe=1"
cpu_bind_rpc1_01="--map-by ppr:1:socket:pe=1"

##############################################################################################################################################
# Large DDR problem, strong scaled across 1 node.

##############################################################################################################################################
# MPI + Pthreads, 1 rank/core, 2 threads/rank.

${aprun} -n 32 ${cpu_bind_rpc1_32} ${voc} ${vtime} ${xpre}${prob}_ddr_nn_0001_nppn_032_ntpp_002.Linux --tpp 2 >& ${prob}_ddr_nn_0001_nppn_032_ntpp_002.log
${aprun} -n 16 ${cpu_bind_rpc1_16} ${voc} ${vtime} ${xpre}${prob}_ddr_nn_0001_nppn_016_ntpp_002.Linux --tpp 2 >& ${prob}_ddr_nn_0001_nppn_016_ntpp_002.log
${aprun} -n  8 ${cpu_bind_rpc1_08} ${voc} ${vtime} ${xpre}${prob}_ddr_nn_0001_nppn_008_ntpp_002.Linux --tpp 2 >& ${prob}_ddr_nn_0001_nppn_008_ntpp_002.log
${aprun} -n  4 ${cpu_bind_rpc1_04} ${voc} ${vtime} ${xpre}${prob}_ddr_nn_0001_nppn_004_ntpp_002.Linux --tpp 2 >& ${prob}_ddr_nn_0001_nppn_004_ntpp_002.log
${aprun} -n  2 ${cpu_bind_rpc1_02} ${voc} ${vtime} ${xpre}${prob}_ddr_nn_0001_nppn_002_ntpp_002.Linux --tpp 2 >& ${prob}_ddr_nn_0001_nppn_002_ntpp_002.log
${aprun} -n  1 ${cpu_bind_rpc1_01} ${voc} ${vtime} ${xpre}${prob}_ddr_nn_0001_nppn_001_ntpp_002.Linux --tpp 2 >& ${prob}_ddr_nn_0001_nppn_001_ntpp_002.log

##############################################################################################################################################
# MPI + Pthreads, 1 rank/core, 1 thread/rank.

${aprun} -n 32 ${cpu_bind_rpc1_32} ${voc} ${vtime} ${xpre}${prob}_ddr_nn_0001_nppn_032_ntpp_001.Linux --tpp 1 >& ${prob}_ddr_nn_0001_nppn_032_ntpp_001.log
${aprun} -n 16 ${cpu_bind_rpc1_16} ${voc} ${vtime} ${xpre}${prob}_ddr_nn_0001_nppn_016_ntpp_001.Linux --tpp 1 >& ${prob}_ddr_nn_0001_nppn_016_ntpp_001.log
${aprun} -n  8 ${cpu_bind_rpc1_08} ${voc} ${vtime} ${xpre}${prob}_ddr_nn_0001_nppn_008_ntpp_001.Linux --tpp 1 >& ${prob}_ddr_nn_0001_nppn_008_ntpp_001.log
${aprun} -n  4 ${cpu_bind_rpc1_04} ${voc} ${vtime} ${xpre}${prob}_ddr_nn_0001_nppn_004_ntpp_001.Linux --tpp 1 >& ${prob}_ddr_nn_0001_nppn_004_ntpp_001.log
${aprun} -n  2 ${cpu_bind_rpc1_02} ${voc} ${vtime} ${xpre}${prob}_ddr_nn_0001_nppn_002_ntpp_001.Linux --tpp 1 >& ${prob}_ddr_nn_0001_nppn_002_ntpp_001.log
${aprun} -n  1 ${cpu_bind_rpc1_01} ${voc} ${vtime} ${xpre}${prob}_ddr_nn_0001_nppn_001_ntpp_001.Linux --tpp 1 >& ${prob}_ddr_nn_0001_nppn_001_ntpp_001.log

##############################################################################################################################################
# Small HBM problem, strong scaled across 1 node.

##############################################################################################################################################
# MPI + Pthreads, 1 rank/core, 2 threads/rank.

${aprun} -n 32 ${cpu_bind_rpc1_32} ${voc} ${vtime} ${xpre}${prob}_hbm_nn_0001_nppn_032_ntpp_002.Linux --tpp 2 >& ${prob}_hbm_nn_0001_nppn_032_ntpp_002.log
${aprun} -n 16 ${cpu_bind_rpc1_16} ${voc} ${vtime} ${xpre}${prob}_hbm_nn_0001_nppn_016_ntpp_002.Linux --tpp 2 >& ${prob}_hbm_nn_0001_nppn_016_ntpp_002.log
${aprun} -n  8 ${cpu_bind_rpc1_08} ${voc} ${vtime} ${xpre}${prob}_hbm_nn_0001_nppn_008_ntpp_002.Linux --tpp 2 >& ${prob}_hbm_nn_0001_nppn_008_ntpp_002.log
${aprun} -n  4 ${cpu_bind_rpc1_04} ${voc} ${vtime} ${xpre}${prob}_hbm_nn_0001_nppn_004_ntpp_002.Linux --tpp 2 >& ${prob}_hbm_nn_0001_nppn_004_ntpp_002.log
${aprun} -n  2 ${cpu_bind_rpc1_02} ${voc} ${vtime} ${xpre}${prob}_hbm_nn_0001_nppn_002_ntpp_002.Linux --tpp 2 >& ${prob}_hbm_nn_0001_nppn_002_ntpp_002.log
${aprun} -n  1 ${cpu_bind_rpc1_01} ${voc} ${vtime} ${xpre}${prob}_hbm_nn_0001_nppn_001_ntpp_002.Linux --tpp 2 >& ${prob}_hbm_nn_0001_nppn_001_ntpp_002.log

##############################################################################################################################################
# MPI + Pthreads, 1 rank/core, 1 thread/rank.

${aprun} -n 32 ${cpu_bind_rpc1_32} ${voc} ${vtime} ${xpre}${prob}_hbm_nn_0001_nppn_032_ntpp_001.Linux --tpp 1 >& ${prob}_hbm_nn_0001_nppn_032_ntpp_001.log
${aprun} -n 16 ${cpu_bind_rpc1_16} ${voc} ${vtime} ${xpre}${prob}_hbm_nn_0001_nppn_016_ntpp_001.Linux --tpp 1 >& ${prob}_hbm_nn_0001_nppn_016_ntpp_001.log
${aprun} -n  8 ${cpu_bind_rpc1_08} ${voc} ${vtime} ${xpre}${prob}_hbm_nn_0001_nppn_008_ntpp_001.Linux --tpp 1 >& ${prob}_hbm_nn_0001_nppn_008_ntpp_001.log
${aprun} -n  4 ${cpu_bind_rpc1_04} ${voc} ${vtime} ${xpre}${prob}_hbm_nn_0001_nppn_004_ntpp_001.Linux --tpp 1 >& ${prob}_hbm_nn_0001_nppn_004_ntpp_001.log
${aprun} -n  2 ${cpu_bind_rpc1_02} ${voc} ${vtime} ${xpre}${prob}_hbm_nn_0001_nppn_002_ntpp_001.Linux --tpp 1 >& ${prob}_hbm_nn_0001_nppn_002_ntpp_001.log
${aprun} -n  1 ${cpu_bind_rpc1_01} ${voc} ${vtime} ${xpre}${prob}_hbm_nn_0001_nppn_001_ntpp_001.Linux --tpp 1 >& ${prob}_hbm_nn_0001_nppn_001_ntpp_001.log
