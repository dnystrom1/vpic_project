#!/bin/bash

##############################################################################################################################################
# Configure modules.

source bashrc.modules

##############################################################################################################################################
# Figure out problem being run based on value of PWD variable.

if echo $PWD | grep -q "lpi"
then
    prob=lpi
fi

if echo $PWD | grep -q "lpi_sp"
then
    prob=lpi_sp
fi

if echo $PWD | grep -q "reconnection"
then
    prob=reconnection
fi

if [ "x$prob" = "x" ]
then
    echo "The prob bash variable is not set to a valid problem."
    exit 1
fi

##############################################################################################################################################
# Preliminary stuff.

vtime=''

#voc='--vm-overcommit=enable'
voc=''

##############################################################################################################################################
# Configure aprun and xpre variables based on type of run.

############################################################
# Configure default run without extra tools.

xpre='./'

aprun='mpirun'

############################################################
# Configure run with mpitrace.

#xpre='./'

#aprun='mpirun'

#ARCH=DARWIN_INT_SKY_OMPI_GNU_OPT_ARTEAM
#PROJECTS_ROOT=/home/wdn/Projects
#MPITRACE_PROJECT_ROOT=$PROJECTS_ROOT/mpitrace_walkup_project
#MPITRACE_ROOT=$MPITRACE_PROJECT_ROOT/build/$ARCH/mpitrace_walkup

#source $MPITRACE_ROOT/bashrc.modules

# Case 1
#export VPROF_PROFILE=yes
#export LD_PRELOAD=$MPITRACE_ROOT/src/libmpitrace.so

# Case 2
#export HPM_PROFILE_THRESHOLD=2000000
#export HPM_PROFILE_EVENT=""
#export LD_PRELOAD=$MPITRACE_ROOT/src/libmpihpm.so

# Case 3
#export HPM_PROFILE_THRESHOLD=2000000
#export HPM_PROFILE_EVENT=""
#export LD_PRELOAD=$MPITRACE_ROOT/hpmprof/libhpmprof.so

############################################################
# Configure MAP profiling collection run.

#xpre='./'

#aprun='map --profile mpirun'

############################################################
# Configure Intel Application Performance Snapshot run.

#xpre='aps ./'

#aprun='mpirun'

##############################################################################################################################################
# Define some useful variables to tame the command lines when using the low level process binding interface.

cpu_list_rpc1_56_socket_0="0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27"
cpu_list_rpc1_56_socket_1="28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55"

cpu_list_rpc1_32_socket_0="0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15"
cpu_list_rpc1_32_socket_1="28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43"

cpu_list_rpc1_16_socket_0="0,1,2,3,4,5,6,7"
cpu_list_rpc1_16_socket_1="28,29,30,31,32,33,34,35"

cpu_list_rpc1_08_socket_0="0,1,2,3"
cpu_list_rpc1_08_socket_1="28,29,30,31"

cpu_list_rpc1_04_socket_0="0,1"
cpu_list_rpc1_04_socket_1="28,29"

cpu_list_rpc1_02_socket_0="0"
cpu_list_rpc1_02_socket_1="28"

cpu_list_rpc1_01_socket_0="0"

#cpu_bind_rpc1_56="--cpu_bind=map_cpu:${cpu_list_rpc1_56_socket_0},${cpu_list_rpc1_56_socket_1}"
#cpu_bind_rpc1_32="--cpu_bind=map_cpu:${cpu_list_rpc1_32_socket_0},${cpu_list_rpc1_32_socket_1}"
#cpu_bind_rpc1_16="--cpu_bind=map_cpu:${cpu_list_rpc1_16_socket_0},${cpu_list_rpc1_16_socket_1}"
#cpu_bind_rpc1_08="--cpu_bind=map_cpu:${cpu_list_rpc1_08_socket_0},${cpu_list_rpc1_08_socket_1}"
#cpu_bind_rpc1_04="--cpu_bind=map_cpu:${cpu_list_rpc1_04_socket_0},${cpu_list_rpc1_04_socket_1}"
#cpu_bind_rpc1_02="--cpu_bind=map_cpu:${cpu_list_rpc1_02_socket_0},${cpu_list_rpc1_02_socket_1}"
#cpu_bind_rpc1_01="--cpu_bind=map_cpu:${cpu_list_rpc1_01_socket_0}"

cpu_bind_rpc1_56="--map-by ppr:28:socket:pe=1"
cpu_bind_rpc1_32="--map-by ppr:16:socket:pe=1"
cpu_bind_rpc1_16="--map-by ppr:8:socket:pe=1"
cpu_bind_rpc1_08="--map-by ppr:4:socket:pe=1"
cpu_bind_rpc1_04="--map-by ppr:2:socket:pe=1"
cpu_bind_rpc1_02="--map-by ppr:1:socket:pe=1"
cpu_bind_rpc1_01="--map-by ppr:1:socket:pe=1"

##############################################################################################################################################
# Large DDR problem, strong scaled across 1 node.

##############################################################################################################################################
# MPI + Pthreads, 1 rank/core, 2 threads/rank.

${aprun} -n 56 ${cpu_bind_rpc1_56} ${voc} ${vtime} ${xpre}${prob}_ddr_nn_0001_nppn_056_ntpp_002.Linux --tpp 2 >& ${prob}_ddr_nn_0001_nppn_056_ntpp_002.log
${aprun} -n 32 ${cpu_bind_rpc1_32} ${voc} ${vtime} ${xpre}${prob}_ddr_nn_0001_nppn_032_ntpp_002.Linux --tpp 2 >& ${prob}_ddr_nn_0001_nppn_032_ntpp_002.log
${aprun} -n 16 ${cpu_bind_rpc1_16} ${voc} ${vtime} ${xpre}${prob}_ddr_nn_0001_nppn_016_ntpp_002.Linux --tpp 2 >& ${prob}_ddr_nn_0001_nppn_016_ntpp_002.log
${aprun} -n  8 ${cpu_bind_rpc1_08} ${voc} ${vtime} ${xpre}${prob}_ddr_nn_0001_nppn_008_ntpp_002.Linux --tpp 2 >& ${prob}_ddr_nn_0001_nppn_008_ntpp_002.log
${aprun} -n  4 ${cpu_bind_rpc1_04} ${voc} ${vtime} ${xpre}${prob}_ddr_nn_0001_nppn_004_ntpp_002.Linux --tpp 2 >& ${prob}_ddr_nn_0001_nppn_004_ntpp_002.log
${aprun} -n  2 ${cpu_bind_rpc1_02} ${voc} ${vtime} ${xpre}${prob}_ddr_nn_0001_nppn_002_ntpp_002.Linux --tpp 2 >& ${prob}_ddr_nn_0001_nppn_002_ntpp_002.log
${aprun} -n  1 ${cpu_bind_rpc1_01} ${voc} ${vtime} ${xpre}${prob}_ddr_nn_0001_nppn_001_ntpp_002.Linux --tpp 2 >& ${prob}_ddr_nn_0001_nppn_001_ntpp_002.log

##############################################################################################################################################
# MPI + Pthreads, 1 rank/core, 1 thread/rank.

${aprun} -n 56 ${cpu_bind_rpc1_56} ${voc} ${vtime} ${xpre}${prob}_ddr_nn_0001_nppn_056_ntpp_001.Linux --tpp 1 >& ${prob}_ddr_nn_0001_nppn_056_ntpp_001.log
${aprun} -n 32 ${cpu_bind_rpc1_32} ${voc} ${vtime} ${xpre}${prob}_ddr_nn_0001_nppn_032_ntpp_001.Linux --tpp 1 >& ${prob}_ddr_nn_0001_nppn_032_ntpp_001.log
${aprun} -n 16 ${cpu_bind_rpc1_16} ${voc} ${vtime} ${xpre}${prob}_ddr_nn_0001_nppn_016_ntpp_001.Linux --tpp 1 >& ${prob}_ddr_nn_0001_nppn_016_ntpp_001.log
${aprun} -n  8 ${cpu_bind_rpc1_08} ${voc} ${vtime} ${xpre}${prob}_ddr_nn_0001_nppn_008_ntpp_001.Linux --tpp 1 >& ${prob}_ddr_nn_0001_nppn_008_ntpp_001.log
${aprun} -n  4 ${cpu_bind_rpc1_04} ${voc} ${vtime} ${xpre}${prob}_ddr_nn_0001_nppn_004_ntpp_001.Linux --tpp 1 >& ${prob}_ddr_nn_0001_nppn_004_ntpp_001.log
${aprun} -n  2 ${cpu_bind_rpc1_02} ${voc} ${vtime} ${xpre}${prob}_ddr_nn_0001_nppn_002_ntpp_001.Linux --tpp 1 >& ${prob}_ddr_nn_0001_nppn_002_ntpp_001.log
${aprun} -n  1 ${cpu_bind_rpc1_01} ${voc} ${vtime} ${xpre}${prob}_ddr_nn_0001_nppn_001_ntpp_001.Linux --tpp 1 >& ${prob}_ddr_nn_0001_nppn_001_ntpp_001.log

##############################################################################################################################################
# Small HBM problem, strong scaled across 1 node.

##############################################################################################################################################
# MPI + Pthreads, 1 rank/core, 2 threads/rank.

${aprun} -n 56 ${cpu_bind_rpc1_56} ${voc} ${vtime} ${xpre}${prob}_hbm_nn_0001_nppn_056_ntpp_002.Linux --tpp 2 >& ${prob}_hbm_nn_0001_nppn_056_ntpp_002.log
${aprun} -n 32 ${cpu_bind_rpc1_32} ${voc} ${vtime} ${xpre}${prob}_hbm_nn_0001_nppn_032_ntpp_002.Linux --tpp 2 >& ${prob}_hbm_nn_0001_nppn_032_ntpp_002.log
${aprun} -n 16 ${cpu_bind_rpc1_16} ${voc} ${vtime} ${xpre}${prob}_hbm_nn_0001_nppn_016_ntpp_002.Linux --tpp 2 >& ${prob}_hbm_nn_0001_nppn_016_ntpp_002.log
${aprun} -n  8 ${cpu_bind_rpc1_08} ${voc} ${vtime} ${xpre}${prob}_hbm_nn_0001_nppn_008_ntpp_002.Linux --tpp 2 >& ${prob}_hbm_nn_0001_nppn_008_ntpp_002.log
${aprun} -n  4 ${cpu_bind_rpc1_04} ${voc} ${vtime} ${xpre}${prob}_hbm_nn_0001_nppn_004_ntpp_002.Linux --tpp 2 >& ${prob}_hbm_nn_0001_nppn_004_ntpp_002.log
${aprun} -n  2 ${cpu_bind_rpc1_02} ${voc} ${vtime} ${xpre}${prob}_hbm_nn_0001_nppn_002_ntpp_002.Linux --tpp 2 >& ${prob}_hbm_nn_0001_nppn_002_ntpp_002.log
${aprun} -n  1 ${cpu_bind_rpc1_01} ${voc} ${vtime} ${xpre}${prob}_hbm_nn_0001_nppn_001_ntpp_002.Linux --tpp 2 >& ${prob}_hbm_nn_0001_nppn_001_ntpp_002.log

##############################################################################################################################################
# MPI + Pthreads, 1 rank/core, 1 thread/rank.

${aprun} -n 56 ${cpu_bind_rpc1_56} ${voc} ${vtime} ${xpre}${prob}_hbm_nn_0001_nppn_056_ntpp_001.Linux --tpp 1 >& ${prob}_hbm_nn_0001_nppn_056_ntpp_001.log
${aprun} -n 32 ${cpu_bind_rpc1_32} ${voc} ${vtime} ${xpre}${prob}_hbm_nn_0001_nppn_032_ntpp_001.Linux --tpp 1 >& ${prob}_hbm_nn_0001_nppn_032_ntpp_001.log
${aprun} -n 16 ${cpu_bind_rpc1_16} ${voc} ${vtime} ${xpre}${prob}_hbm_nn_0001_nppn_016_ntpp_001.Linux --tpp 1 >& ${prob}_hbm_nn_0001_nppn_016_ntpp_001.log
${aprun} -n  8 ${cpu_bind_rpc1_08} ${voc} ${vtime} ${xpre}${prob}_hbm_nn_0001_nppn_008_ntpp_001.Linux --tpp 1 >& ${prob}_hbm_nn_0001_nppn_008_ntpp_001.log
${aprun} -n  4 ${cpu_bind_rpc1_04} ${voc} ${vtime} ${xpre}${prob}_hbm_nn_0001_nppn_004_ntpp_001.Linux --tpp 1 >& ${prob}_hbm_nn_0001_nppn_004_ntpp_001.log
${aprun} -n  2 ${cpu_bind_rpc1_02} ${voc} ${vtime} ${xpre}${prob}_hbm_nn_0001_nppn_002_ntpp_001.Linux --tpp 1 >& ${prob}_hbm_nn_0001_nppn_002_ntpp_001.log
${aprun} -n  1 ${cpu_bind_rpc1_01} ${voc} ${vtime} ${xpre}${prob}_hbm_nn_0001_nppn_001_ntpp_001.Linux --tpp 1 >& ${prob}_hbm_nn_0001_nppn_001_ntpp_001.log
