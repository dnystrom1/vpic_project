#!/bin/bash

##############################################################################################################################################
# Configure modules.

source bashrc.modules

##############################################################################################################################################
# Figure out problem being run based on value of PWD variable.

if echo $PWD | grep -q "lpi"
then
    prob=lpi
fi

if echo $PWD | grep -q "reconnection"
then
    prob=reconnection
fi

if [ "x$prob" = "x" ]
then
    echo "The prob bash variable is not set to a valid problem."
    exit 1
fi

##############################################################################################################################################
# Preliminary stuff.

vtime=''

#voc='--vm-overcommit=enable'
voc=''

##############################################################################################################################################
# Configure aprun and xpre variables based on type of run.

############################################################
# Configure default run without extra tools.

xpre='./'

aprun='srun'

############################################################
# Configure CrayPat-lite run.

#xpre='./'

#aprun='srun'

############################################################
# Configure MAP profiling collection run.

#xpre='./'

#aprun='map --profile srun'

############################################################
# Configure Intel Application Performance Snapshot run.

#xpre='aps ./'

#aprun='srun'

##############################################################################################################################################
# Define some useful variables to tame the command lines when using the low level process binding interface.

cpu_list_rpc1_16_socket_0="0,1,2,3,4,5,6,7"
cpu_list_rpc1_16_socket_1="8,9,10,11,12,13,14,15"

cpu_list_rpc1_08_socket_0="0,1,2,3"
cpu_list_rpc1_08_socket_1="8,9,10,11"

cpu_list_rpc1_04_socket_0="0,1"
cpu_list_rpc1_04_socket_1="8,9"

cpu_list_rpc1_02_socket_0="0"
cpu_list_rpc1_02_socket_1="8"

cpu_list_rpc1_01_socket_0="0"

cpu_bind_rpc1_16="--cpu_bind=map_cpu:${cpu_list_rpc1_16_socket_0},${cpu_list_rpc1_16_socket_1}"
cpu_bind_rpc1_08="--cpu_bind=map_cpu:${cpu_list_rpc1_08_socket_0},${cpu_list_rpc1_08_socket_1}"
cpu_bind_rpc1_04="--cpu_bind=map_cpu:${cpu_list_rpc1_04_socket_0},${cpu_list_rpc1_04_socket_1}"
cpu_bind_rpc1_02="--cpu_bind=map_cpu:${cpu_list_rpc1_02_socket_0},${cpu_list_rpc1_02_socket_1}"
cpu_bind_rpc1_01="--cpu_bind=map_cpu:${cpu_list_rpc1_01_socket_0}"

##############################################################################################################################################
# Large DDR problem, strong scaled across 8 nodes.

##############################################################################################################################################
# MPI only, 1 rank/core.

${aprun} -n 128 -N 8 ${cpu_bind_rpc1_16} ${voc} ${vtime} ${xpre}${prob}_ddr_nn_0008_nppn_016_ntpp_001.Linux --tpp 1 >& ${prob}_ddr_nn_0008_nppn_016_nrpc_001.log
${aprun} -n  64 -N 4 ${cpu_bind_rpc1_16} ${voc} ${vtime} ${xpre}${prob}_ddr_nn_0004_nppn_016_ntpp_001.Linux --tpp 1 >& ${prob}_ddr_nn_0004_nppn_016_nrpc_001.log
${aprun} -n  32 -N 2 ${cpu_bind_rpc1_16} ${voc} ${vtime} ${xpre}${prob}_ddr_nn_0002_nppn_016_ntpp_001.Linux --tpp 1 >& ${prob}_ddr_nn_0002_nppn_016_nrpc_001.log

${aprun} -n  16 -N 1 ${cpu_bind_rpc1_16} ${voc} ${vtime} ${xpre}${prob}_ddr_nn_0001_nppn_016_ntpp_001.Linux --tpp 1 >& ${prob}_ddr_nn_0001_nppn_016_nrpc_001.log
${aprun} -n   8 -N 1 ${cpu_bind_rpc1_08} ${voc} ${vtime} ${xpre}${prob}_ddr_nn_0001_nppn_008_ntpp_001.Linux --tpp 1 >& ${prob}_ddr_nn_0001_nppn_008_nrpc_001.log
${aprun} -n   4 -N 1 ${cpu_bind_rpc1_04} ${voc} ${vtime} ${xpre}${prob}_ddr_nn_0001_nppn_004_ntpp_001.Linux --tpp 1 >& ${prob}_ddr_nn_0001_nppn_004_nrpc_001.log
${aprun} -n   2 -N 1 ${cpu_bind_rpc1_02} ${voc} ${vtime} ${xpre}${prob}_ddr_nn_0001_nppn_002_ntpp_001.Linux --tpp 1 >& ${prob}_ddr_nn_0001_nppn_002_nrpc_001.log
${aprun} -n   1 -N 1 ${cpu_bind_rpc1_01} ${voc} ${vtime} ${xpre}${prob}_ddr_nn_0001_nppn_001_ntpp_001.Linux --tpp 1 >& ${prob}_ddr_nn_0001_nppn_001_nrpc_001.log

##############################################################################################################################################
# Small HBM problem, strong scaled across 1 node.

##############################################################################################################################################
# MPI only, 1 rank/core.

${aprun} -n  16 -N 1 ${cpu_bind_rpc1_16} ${voc} ${vtime} ${xpre}${prob}_hbm_nn_0001_nppn_016_ntpp_001.Linux --tpp 1 >& ${prob}_hbm_nn_0001_nppn_016_nrpc_001.log
${aprun} -n   8 -N 1 ${cpu_bind_rpc1_08} ${voc} ${vtime} ${xpre}${prob}_hbm_nn_0001_nppn_008_ntpp_001.Linux --tpp 1 >& ${prob}_hbm_nn_0001_nppn_008_nrpc_001.log
${aprun} -n   4 -N 1 ${cpu_bind_rpc1_04} ${voc} ${vtime} ${xpre}${prob}_hbm_nn_0001_nppn_004_ntpp_001.Linux --tpp 1 >& ${prob}_hbm_nn_0001_nppn_004_nrpc_001.log
${aprun} -n   2 -N 1 ${cpu_bind_rpc1_02} ${voc} ${vtime} ${xpre}${prob}_hbm_nn_0001_nppn_002_ntpp_001.Linux --tpp 1 >& ${prob}_hbm_nn_0001_nppn_002_nrpc_001.log
${aprun} -n   1 -N 1 ${cpu_bind_rpc1_01} ${voc} ${vtime} ${xpre}${prob}_hbm_nn_0001_nppn_001_ntpp_001.Linux --tpp 1 >& ${prob}_hbm_nn_0001_nppn_001_nrpc_001.log
